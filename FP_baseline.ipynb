{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final: Baseline\n",
    "\n",
    "O projeto consiste de detecção de sarcasmo em manchetes a partir de duas fontes: \"HuffingtonPost\" para manchetes confiáveis e \"The Onion\" para manchetes sarcásticas. O resultado a seguir é apenas inicial, um baseline, para depois ser aprimorado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import join\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para auxílio no processamento de linguagem natural, será utilizada a biblioteca _NLTK_. É uma biblioteca muito grande, mas felizmente não serão necessários todos os módulos.\n",
    "\n",
    "**TODO: Adicionar download dos módulos no código.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'Dataset'\n",
    "dataset = pd.read_json(join(folder, 'Sarcasm_Headlines_Dataset_v2.json'), lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para melhor análise dos dados, devemos dividir cada manchete em palavras, chamadas de _tokens_ . Assim, podemos processar melhor o texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [thirtysomething, scientists, unveil, doomsday...\n",
      "1        [dem, rep., totally, nails, why, congress, is,...\n",
      "2        [eat, your, veggies, :, 9, deliciously, differ...\n",
      "3        [inclement, weather, prevents, liar, from, get...\n",
      "4        [mother, comes, pretty, close, to, using, word...\n",
      "                               ...                        \n",
      "28614    [jews, to, celebrate, rosh, hashasha, or, some...\n",
      "28615    [internal, affairs, investigator, disappointed...\n",
      "28616    [the, most, beautiful, acceptance, speech, thi...\n",
      "28617    [mars, probe, destroyed, by, orbiting, spielbe...\n",
      "28618           [dad, clarifies, this, not, a, food, stop]\n",
      "Name: headline, Length: 28619, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenize headlines\n",
    "token_head = dataset['headline'].apply(word_tokenize)\n",
    "print(token_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em uma frase, normalmente existem palavras comuns que não contribuem tanto para o significado. No inglês, um exemplo é a palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [thirtysomething, scientists, unveil, doomsday...\n",
      "1        [dem, rep., totally, nails, congress, falling,...\n",
      "2        [eat, veggies, :, 9, deliciously, different, r...\n",
      "3        [inclement, weather, prevents, liar, getting, ...\n",
      "4        [mother, comes, pretty, close, using, word, 's...\n",
      "                               ...                        \n",
      "28614         [jews, celebrate, rosh, hashasha, something]\n",
      "28615    [internal, affairs, investigator, disappointed...\n",
      "28616    [beautiful, acceptance, speech, week, came, qu...\n",
      "28617    [mars, probe, destroyed, orbiting, spielberg-g...\n",
      "28618                         [dad, clarifies, food, stop]\n",
      "Name: headline, Length: 28619, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords: common words that are less useful for detection (example:\"the\")\n",
    "stop = set(stopwords.words('english'))\n",
    "filt = token_head.apply(lambda row: list(filter(lambda w: w not in stop, row)))\n",
    "dataset['headline'] = filt\n",
    "print(filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27704    [wanted, government, run, like, business, ?, g...\n",
      "24834          [hair, salon, acquires, rare, nagel, print]\n",
      "11360                     [cbs, picks, nbc, nightly, news]\n",
      "11243    [rotating, knife, vortex, closed, pending, saf...\n",
      "6695            [boardroom, hokey, pokey, :, dance, women]\n",
      "                               ...                        \n",
      "27430    [nation, 's, dogs, vow, keep, shit, together, ...\n",
      "8316                   [30, reasons, give, thanks, horses]\n",
      "22066    [princess, nokia, reveals, threw, soup, racist...\n",
      "7170     [olympic, bronze, medalist, appear, flintstone...\n",
      "9659             [9, parking, garage, designs, works, art]\n",
      "Name: headline, Length: 25757, dtype: object\n",
      "27704    0\n",
      "24834    1\n",
      "11360    1\n",
      "11243    1\n",
      "6695     0\n",
      "        ..\n",
      "27430    1\n",
      "8316     0\n",
      "22066    0\n",
      "7170     1\n",
      "9659     0\n",
      "Name: is_sarcastic, Length: 25757, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset.\n",
    "X, X_test, Y, Y_test = train_test_split(dataset['headline'], dataset['is_sarcastic'], test_size=0.1)\n",
    "\n",
    "print(X)\n",
    "print(Y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
